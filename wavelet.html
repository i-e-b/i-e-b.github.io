<!DOCTYPE html>
<html lang="en">
<head>
    <title>i-e-b: Wavelet image compression</title>
</head>
<body>
    <h1>Wavelet image compression</h1>
    <p>General notes from experiments using wavelets for image compression.</p>
    <p>The purpose of the project is to make a file format for images that meets these goals:</p>
    <ul>
        <li>Compete with PNG and JPG in terms of file size for a given image quality</li>
        <li>Support reconstruction of a partial image from a truncated data source</li>
        <li>Support animations / video with minimal complexity</li>
        <li>Support arbitrary color spaces / channel counts</li>
    </ul>
    <p>Non-goals:</p>
    <ul>
        <li>Work in constrained memory</li>
        <li>Be best-in-class for speed or resulting file size</li>
    </ul>
    <p>Code for these experiments is in the project at <a href="https://github.com/i-e-b/ImageTools">https://github.com/i-e-b/ImageTools</a></p>

    <a name="FreqDecom"><h2>Frequency Decomposition</h2></a>
    <!--Stuff about avoiding HH quarter that the standard docs use-->
    <h3>Planar separation</h3>
    <p>The wavelet transform is inherently 1 dimensional. There are a few different ways to apply this to planar images (2D) and video (3D).
    I have looked into planar separation (applying the transform alternately to each individual row and column of the image) and spacefilling curve transforms (where we transform the image into a single 1 dimensional array and apply to transform in a single pass).</p>

    <h4>Planar 3</h4>
    <p>The method I've seen in most work (including JPEG2000) is what I'll call <i>Planar 3</i>, as it results in 3 sets of wavelet coefficients
    per decomposition round.</p>
    <img src="img/wave_planar3_seq.png" alt="decomposition sequence of Planar 3, which results in LH, HH, HL sets."/>
    <img src="img/wave_planar3_struct.png" alt="data structure of Planar 3, showing 3 sets of coefficients per round"/>
    <p>This includes a double-decomposition of coefficients in the HH region (marked 1, 4, 7 in the structure image).
    My tests show that this both increases computation and reduces compression efficiency (i.e. gives larger file sizes for the
    same input)</p>
    <h4>Planar 2</h4>
    <p>Instead, I propose 'Planar 2', which produces 2 sets of wavelet coefficients per decomposition round.</p>
    <img src="img/wave_planar2_seq.png" alt="decomposition sequence of Planar 2, which results in LH, HL sets."/>
    <img src="img/wave_planar2_struct.png" alt="data structure of Planar 2, showing 2 sets of coefficients per round"/>
    <p>This reduces computation of the wavelet transform by 0.25, as the HH section is not calculated. It also improves
    resulting file size: Using test image 3 and no quantisation <tt>Planar 3 = 691kb</tt>, <tt>Planar 2 = 655kb</tt>, or about 5% improvement.</p>

    <h3>Space filling curves</h3>
    <p>I experimented using the 'Z order' and 'Hilbert' curves as spatial transforms. Both of these added significant processing time to perform the transform,
    introduced significant compression artifacts during quantisation, and resulted in overall larger image sizes. I would consider this a strong failure.</p>
    <p>Using test image 3 and no quantisation: <tt>Planar 2 = 655kb</tt>, <tt>Z-order = 871kb</tt></p>
    <img src="img/wave_zorder_sfc.png" alt="Morton, or Z-order curve, showing sample order and round boundaries"/>
    <img src="img/wave_hilbert_sfc.png" alt="Hilbert curve, showing sample points and round boundaries"/>
    <img src="img/wave_morton_quant.png" title="Example of z-order quantisation artefacts"/>

    <a name="quant"><h2>Quantisation</h2></a>
    <p>Quantisation of wavelet coefficients has the greatest controllable effect on file size and image quality. For images that contain large blocks of similar value (the kind that perform well with PNG images) quantisation gives minimal gains. For complex natural images, quantisation gives very great reduction of file size with a corresponding reduction in image quality. We apply different quantisation levels to each channel -- reducing Luma the least, and chroma channels significantly. We do not sub-sample chroma directly, as the transform plus quantisation achieves the same effect but with the benefit of being adaptive.</p>
    <p>Multiple different schemes of quantisation were tried, but by far the best effect was from a simple linear divide/multiply.</p>
    <p>We apply a different quantisation factor to each round of the wavelet decomposition. The highest frequencies can take the most reduction, whereas the lowest frequencies show very significant degredation even at small reductions.</p>
    <img src="img/wave_quant_strong.png" title="Example of heavy quantisation (0.26bpp)"/>
    <img src="img/wave_quant_weak.png" title="Example of slight quantisation (1.94bpp)"/>
    <img src="img/wave_quant_chroma.png" title="Example of heavy quantisation only on chroma channels (2.45bpp)"/>

    <a name="CoefLayout"><h2>Coefficient Layout for storage</h2></a>
    <p>Reordering by frequency scale to avoid mixing entropy, and for truncation support</p>

    <a name="primary"><h2>Primary encoding</h2></a>
    <p>Fibonacci encoding, plus the failed variants</p>

    <a name="entropy"><h2>Entropy encoding</h2></a>
    <p>Deflate, plus notes on markov arithmetic coding</p>
    <p>Example 3 with quantisation: <tt>Deflate = 251.97kb (1.97 bpp)</tt>, <tt>Markov = 248.54kb (1.94 bpp)</tt></p>

    <a name="partial"><h2>Partial recovery</h2></a>
    <p>Decoding a smaller scale image (needs gamma correction), decoding a truncated stream (incl. interleaved channel streams)</p>


    <a name="3dimg"><h2>3D images</h2></a>
    <p>TODO: decomposition overlap, order invariance depth-first or plane-first (and encoding impact), temporal ringing.</p>
</body>
</html>
